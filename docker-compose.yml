services:
  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8000:8000"
    volumes:
      - chroma_data:/data
    environment:
      - ANONYMIZED_TELEMETRY=false

  # Ollama uses the GPU to run the models in linux
  ollama:
    image: ollama/ollama
    profiles: ["linux"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ollama_storage:/root/.ollama

  api-dev:
    build:
      context: .
      dockerfile: Dockerfile.dev
    ports:
      - "3001:3001"
    depends_on:
      - chromadb
    env_file:
      - ${ENV_FILE:-.env.mac}
    volumes:
      - ./src:/app/src:ro
      - ./data:/app/data:ro
    develop:
      watch:
        - action: sync
          path: ./src
          target: /app/src
    command: npx tsx watch src/index.ts

  api:
    build: .
    ports:
      - "3001:3001"
    depends_on:
      - chromadb
    env_file:
      - ${ENV_FILE:-.env.mac}
    volumes:
      - ./data:/app/data:ro
    profiles:
      - prod
    command: node dist/index.js

  mcp-agent:
    build: .
    depends_on:
      - chromadb
    env_file:
      - ${ENV_FILE:-.env.mac}
    volumes:
      - ./src:/app/src:ro
    command: npx tsx src/mcp.ts
    ports:
      - "3002:3002"

volumes:
  chroma_data:
  ollama_storage: